# -*- coding: utf-8 -*-
"""Movie's_Customers_Feedbacks_Deep_sentiment_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d3Gxk3zEsO28dos6VDAO8mWAM-vtgBaL

### 1. Import Modules
"""

import numpy as np
import pandas as pd
import nltk
import matplotlib.pyplot as plt
from tensorflow import keras

# download Punkt Sentence Tokenizer
nltk.download('punkt')
# download stopwords
nltk.download('stopwords')

"""### 2. Download and Load Dataset"""

# download IMDB dataset
!wget "https://raw.githubusercontent.com/javaidnabi31/Word-Embeddding-Sentiment-Classification/master/movie_data.csv" -O "movie_data.csv"

# list files in current directory
!ls -lah

# the path to the IMDB dataset
dataset_path = 'movie_data.csv'

# read file (dataset) into our program using pandas
data = pd.read_csv(dataset_path)

# display first 5 rows
data.head()

import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.porter import PorterStemmer

english_stopwords = stopwords.words('english')
stemmer = PorterStemmer()

def clean_review(text):
  # convert to lower case
  text = text.lower()

  # remove none alphabetic characters
  text = re.sub(r'[^a-z]', ' ', text)

  # stem words
  # split into words
  tokens = word_tokenize(text)

  # stemming of words
  stemmed = [stemmer.stem(word) for word in tokens]

  text = ' '.join(stemmed)

  # remove stopwords
  text = ' '.join([word for word in text.split() if word not in english_stopwords])

  return text


# apply to all dataset
data['clean_review'] = data['review'].apply(clean_review)
data.head()

"""### 4. Split Dataset"""

from sklearn.model_selection import train_test_split

X = data['clean_review'].values
y = data['sentiment'].values

# Split data into 50% training & 50% test
# let's all use a random state of 42 for example to ensure having the same split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# define your tokenizer (with num_words=10000)
tokenizer_obj = Tokenizer(num_words=10000)

# assign an index (number) to each word using fit_on_texts function
tokenizer_obj.fit_on_texts(x_train)

# will be used later to pad sequences
max_length = 120

# define vocabulary size
vocab_size = len(tokenizer_obj.word_index) + 1

# transform each text to a sequence of integers (to be used later in embeddings layer)
X_train_tokens =  tokenizer_obj.texts_to_sequences(x_train)
X_test_tokens =  tokenizer_obj.texts_to_sequences(x_test)


# apply post-padding to the sequences
X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')
X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')

x_train[0], X_train_pad[0]

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

embedding_dim = 300

# FILL BLANKS
# build the neural network
model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))
model.add(LSTM(128))
model.add(Dense(1, activation='sigmoid'))

# compile model: assign loss & optimizer
model.compile(loss = 'binary_crossentropy',
              optimizer = 'adam',
              metrics=['accuracy'])

model.summary()

# train model
model.fit(X_train_pad, y_train, batch_size=32, epochs=5, validation_data=(X_test_pad, y_test))

"""### Plot training details

We visualize the training parameters to have a better understanding of the model's convergence.
"""

def plot_accuracy_and_loss(model):
    epochs = model.history.params['epochs']
    epochs = range(epochs)
    val_loss = model.history.history['val_loss']
    val_accuracy = model.history.history['val_accuracy']
    training_loss = model.history.history['loss']
    training_accuracy = model.history.history['accuracy']

    plt.plot(epochs, val_loss, 'r', label='test')
    plt.plot(epochs, training_loss, 'b', label='training')
    plt.xlabel('epochs')
    plt.ylabel('Loss')
    plt.legend(loc='upper right')
    plt.grid(True)
    plt.show()

    plt.plot(epochs, val_accuracy, 'r', label='test')
    plt.plot(epochs, training_accuracy, 'b', label='training')
    plt.xlabel('epochs')
    plt.ylabel('Accuracy')
    plt.legend(loc='lower right')
    plt.grid(True)
    plt.show()

plot_accuracy_and_loss(model)

def predict_sentiment(model, review):
  review = clean_review(review)
  review_tokens = tokenizer_obj.texts_to_sequences([review])
  review_pad = pad_sequences(review_tokens, maxlen=max_length, padding='post')
  prediction = model.predict(review_pad)
  if prediction > 0.5:
    prediction = "positive"
  else:
    prediction = "negative"
  return prediction

review = "This movie is terrible"
predict_sentiment(model, review)

review = "This movie is great"
predict_sentiment(model, review)

review = "a top notch movie"
predict_sentiment(model, review)

review = "a horrible movie"
predict_sentiment(model, review)

model.save('model.h5')

model.evaluate(X_train_pad, y_train)
model.evaluate(X_test_pad, y_test)